#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Section
Introduction
\end_layout

\begin_layout Section
system model
\end_layout

\begin_layout Standard
In this section,we consider a two-tier heterogeneous small cell network.In
 this network,a macrocell base station(MBS) is located in the center of
 it's service area while N self-powered SBS are deployed randomly.The SBSs
 can offload traffic from the MBS in consideration of energy efficiency
 and QoE.
\end_layout

\begin_layout Standard
as shown in Fig.1,the MBS is powered by the traditional power grid while
 the SBSs rely exclusively on energy harvesting sources,e.g, solar energy
 or use wireless power transfer from MBS trasmissions.We consider solar energy
 as SBSs' power source in this paper.To be specific,the energy storage systems(ES
Ss) are used to manager the rondomness of the solar energy coming so that
 the solar energy will be stored in the ESSs and consumed in SBSs' service
 times.
\end_layout

\begin_layout Standard
Without the loss of generality,We assume that the MBS does not interfere
 with SBSs and SBSs will interfere with each other.
 Then,the SINR of MBS is
\begin_inset Formula 
\begin{equation}
SINR_{m}=\frac{P_{m}^{tx}h\left\Vert x\right\Vert ^{-\alpha}}{N_{0}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $p_{m}^{tx}$
\end_inset

is MBS's transmission power,
\begin_inset Formula $h_{x}\sim exp(1)$
\end_inset

 is the transmission fading power between MBS and typical UE at location
 
\begin_inset Formula $x$
\end_inset

, 
\begin_inset Formula $\left\Vert x\right\Vert ^{-\alpha}$
\end_inset

 is the standard path loss function and 
\begin_inset Formula $N_{0}$
\end_inset

donates the noise power.Similarly,the SINR of SBS 
\begin_inset Formula $i$
\end_inset

 is
\begin_inset Formula 
\begin{equation}
SINR_{i}=\frac{P_{i}^{tx}h_{ix}\left\Vert x_{i}\right\Vert ^{-\alpha}}{{\displaystyle \sum_{j=1,j\neq i}^{N}}P_{i}^{tx}h_{ix}\left\Vert x_{i}\right\Vert ^{-\alpha}+N_{0}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $h_{ix}$
\end_inset

 and 
\begin_inset Formula $\left\Vert x_{i}\right\Vert ^{-\alpha}$
\end_inset

donates the transmission fading power and the standard path loss function
 between 
\begin_inset Formula $SBS_{i}$
\end_inset

 and typical UE at location x.
 A typical UE at location 
\begin_inset Formula $x$
\end_inset

 will choose the BS with the maximum SINR(MBS and SBSs are all included)
 to connect to.
\end_layout

\begin_layout Standard
For the BSs' energy consumption, we adopt the model presented in 
\begin_inset CommandInset citation
LatexCommand cite
key "key-1"

\end_inset

.In particular,the power consumption of the MBS can be given by
\begin_inset Formula $P_{m}=(1-q)\rho_{m}^{(t)}P_{m}^{op}+qP_{m}^{op}$
\end_inset

,where 
\begin_inset Formula $\rho_{m}^{(t)}\in[0,1]$
\end_inset

 is the nomalized traffic load of the MBS at time 
\begin_inset Formula $t$
\end_inset

,and 
\begin_inset Formula $P_{m}^{op}$
\end_inset

is the maximum operation power comsumption when the MBS is fully utilized
 and 
\begin_inset Formula $q$
\end_inset

 is a parameter which meature the importance between the fixed power and
 the power related to the normalized load.For the SBS, a constant power consumpti
on model is adopted,
\begin_inset Formula $P_{j}^{(t)}=P_{j}^{op}\sigma_{j}^{(t)}$
\end_inset

, where 
\begin_inset Formula $\sigma_{j}^{(t)}$
\end_inset

 is a utility parameter,e.g.,when the SBS j at time t is on,the 
\begin_inset Formula $\sigma_{j}^{(t)}$
\end_inset

 will equal to one,otherwise it will equal to zero.
\end_layout

\begin_layout Standard
The energy stored in the ESSs can be approximated by the linear function
 
\begin_inset Formula $B_{j}^{(t)}=min(B_{j}^{(t-1)}-P_{j}^{(t)}+E_{j}^{(t)},B_{max})$
\end_inset

,where 
\begin_inset Formula $B_{j}^{(t)}$
\end_inset

stands for the energy stored in the 
\begin_inset Formula $j_{th}$
\end_inset

SBS at time 
\begin_inset Formula $t$
\end_inset

 while 
\begin_inset Formula $B_{j}^{(t)}$
\end_inset

 is the energy coming at time 
\begin_inset Formula $t$
\end_inset

.Then,the total power comsumption is:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
P_{total}^{(t)}=P_{m}^{(t)}+\sum_{j=1}^{N}P_{j}^{(t)}
\end{equation}

\end_inset


\end_layout

\begin_layout Section
BS switching Operation in Deep Reinforcement Learning framework
\end_layout

\begin_layout Subsection
Q-learning 
\end_layout

\begin_layout Standard
we focus on reinforcement learning framework,Q-learning specially, to solve
 the BS switching Operation problem.The Q-learning algorithm consists of
 an agent,state set 
\begin_inset Formula $S$
\end_inset

 ,and a set of action per state 
\begin_inset Formula $A$
\end_inset

 and reward 
\begin_inset Formula $R$
\end_inset

.
 Specially,in our work,the state 
\begin_inset Formula $S$
\end_inset

 can be some observations such as traffic load of the SBSs,the battery level
 of the SBSs and so on.
 The agent choose an action from 
\begin_inset Formula $A$
\end_inset

 acording to the environment so that the agent can move from state to state.Carry
ing out actions in specfic state can get a reward.The goal of this algorighm
 is to maximize the total long-term reward such as the total throughout
 and so on.The optimal action for each state will get the highest reward.Therefore
, the algorithm has an function that calculate the Quantity of the state-action
 pair:
\begin_inset Formula $Q:S\times A\rightarrow\mathbb{R}$
\end_inset

.The 
\begin_inset Formula $Q$
\end_inset

 value of each state-action pair under policy 
\begin_inset Formula $\pi$
\end_inset

 is computed according to the rule: 
\begin_inset Formula 
\begin{equation}
Q(s_{t},a_{t})\leftarrow Q(s_{t},a_{t})+\alpha\left[R_{t}+\gamma\max_{{a}'}Q(s_{t+1},{a}')-Q(s_{t},a_{t})\right]
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\gamma$
\end_inset

 is discounted factor which trades off the importance between the sooner
 reward and later rewards,
\begin_inset Formula $\alpha$
\end_inset

 is the learning rate,
\begin_inset Formula $s_{t+1}$
\end_inset

 is the next state for agent and 
\begin_inset Formula ${a}'$
\end_inset

 is the optimal action according to the policy 
\begin_inset Formula $\pi$
\end_inset

.Then the optimal policy 
\begin_inset Formula $\pi^{*}$
\end_inset

 is 
\begin_inset Formula $\pi^{*}(s)=\arg\max_{a}Q^{\pi^{*}}(s,a),\forall s$
\end_inset

.
\end_layout

\begin_layout Standard
Q-learning is a model-free algorighm,which means one can achieve the optimal
 solution without the prior knowledge about the environment,e.g.the state-transiti
on matrix,which is hard to be known in the complex MDP cases.
 Also,it has been proved that with infinite number of iterations and the
 learning rate decays appropriately,the Q-values will converge with probability
 1 to the 
\begin_inset Formula $Q^{\pi*}$
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "key-2"

\end_inset

.For more details on RL and Q-learning ,reader is referred to ,e.g.,
\begin_inset CommandInset citation
LatexCommand cite
key "key-1"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "key-2"

\end_inset


\end_layout

\begin_layout Subsection
Deep Q-learning
\end_layout

\begin_layout Standard
Althouth Q-learning working very well when the state space and action space
 are small,it suffer from the exponential increasing of the size of Q-value
 table.When state space and action space are large,the Q-value table become
 very large and very hard to converge.Also,Q-learning con not handle the
 situation that the state space is continuous.
\end_layout

\begin_layout Standard
In 2015,Deep Q-learning algorithm which uses a deep neural network instead
 of Q-table to estimitate Q-value was proposed
\begin_inset CommandInset citation
LatexCommand cite
key "key-3"

\end_inset

.(to be continued...)
\end_layout

\begin_layout Section
Performance Evaluation
\end_layout

\begin_layout Standard
In this section,we present the details of our DQN and 
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-1"

\end_inset

Gilsoo Lee, Walid Saad, Mehdi Bennis, Abolfazl Mehbodniya, and Fumiyuki
 ,
\begin_inset Quotes erd
\end_inset

Online Ski Rental for Scheduling Self-Powered Energy Harvesting Small Base
 Stations,
\begin_inset Quotes erd
\end_inset

 in 
\emph on
IEEE International Conference on Communications(ICC),
\emph default
Kuala Lumpur,Malaysia,May.2016
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-1"

\end_inset

R.S.Sutton and A.G.Barto, 
\emph on
Reinforcement Learning:An Introduction
\emph default
.Cambridge,MA:MIT Press,1998
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-2"

\end_inset

C.J.Watkins and P.Dayan,
\begin_inset Quotes erd
\end_inset

Technical note:Q-Learning,
\begin_inset Quotes erd
\end_inset


\emph on
Machine Learning
\emph default
,vol.8,pp.279-292,1992
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-3"

\end_inset

V.
 Mnih, K.
 Kavukcuoglu, D.
 Silver, A.
 Graves, I.
 Antonoglou, D.
 Wier- stra, and M.
 Riedmiller, “Playing atari with deep reinforcement learning,” arXiv preprint
 arXiv:1312.5602, 2013.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-4"

\end_inset


\end_layout

\end_body
\end_document
